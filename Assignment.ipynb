{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Modeling & ETL\n",
    "\n",
    "1. Database Setup\n",
    "Imagine you’re building a dashboard to track orders, customers, and inventory for a food\n",
    "delivery business.\n",
    "Create a simplified schema that includes the following tables:\n",
    "- `Customers`: customer_id, name, registration_date, and region.\n",
    "- `Orders`: order_id, customer_id, order_date, total_amount.\n",
    "- `Items`: item_id, item_name, item_price, and inventory_stock.\n",
    "- `Order_Items`: order_id, item_id, quantity.\n",
    "\n",
    "Deliverable\n",
    "- Provide SQL code for creating these tables.\n",
    "- Write a short paragraph explaining your rationale for the structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "The database design is straightforward and simple, with normalization applied to eliminate data redundancy. Primary keys have been assigned to each table to uniquely identify records and ensure data integrity. This design helps maintain data consistency and reduces redundancy across the database. One potential improvement would be to assign a surrogate primary key to the order_items table. By doing so, we could eliminate the need for a composite key (which currently uses order_id and item_id), improving query performance and making the database easier to maintain over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "-- Create Customers table \n",
    "CREATE TABLE Customers (\n",
    "    customer_id SERIAL PRIMARY KEY,\n",
    "    name VARCHAR(100),\n",
    "    registration_date DATE,\n",
    "    region VARCHAR(50)\n",
    ");\n",
    "\n",
    "-- Create Orders table \n",
    "CREATE TABLE Orders (\n",
    "    order_id SERIAL PRIMARY KEY,\n",
    "    customer_id INT,\n",
    "    order_date DATE,\n",
    "    total_amount DECIMAL(10, 2),\n",
    "    FOREIGN KEY (customer_id) REFERENCES Customers(customer_id)\n",
    ");\n",
    "\n",
    "-- Create Items table \n",
    "CREATE TABLE Items (\n",
    "    item_id SERIAL PRIMARY KEY,\n",
    "    item_name VARCHAR(100),\n",
    "    item_price DECIMAL(10, 2),\n",
    "    inventory_stock INT\n",
    ");\n",
    "\n",
    "-- Create Order_Items table \n",
    "CREATE TABLE Order_Items (\n",
    "    order_id INT,\n",
    "    item_id INT,\n",
    "    quantity INT,\n",
    "    PRIMARY KEY (order_id, item_id),\n",
    "    FOREIGN KEY (order_id) REFERENCES Orders(order_id),\n",
    "    FOREIGN KEY (item_id) REFERENCES Items(item_id)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created successfully!\n",
      "PostgreSQL database tables:\n",
      "customers\n",
      "orders\n",
      "order_items\n",
      "items\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "#Loading environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "#SQL query to check existing tables\n",
    "check_tables_sql = \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\"\n",
    "\n",
    "#Establish connection to Amazon RDS Database\n",
    "try:\n",
    "    \n",
    "    connection = psycopg2.connect(\n",
    "        host=os.getenv(\"DB_HOST\"),\n",
    "        user=os.getenv(\"DB_USER\"),\n",
    "        password=os.getenv(\"DB_PASSWORD\"),\n",
    "        port=os.getenv(\"DB_PORT\"),\n",
    "        dbname=os.getenv(\"DB_NAME\")\n",
    "    )\n",
    "    connection.autocommit = True  \n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(sql)\n",
    "    print(\"Tables created successfully!\")\n",
    "\n",
    "    cursor.execute(check_tables_sql)\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    print(\"PostgreSQL database tables:\")\n",
    "    for table in tables:\n",
    "        print(table[0])\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 records inserted into all tables successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from faker import Faker\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#Initialize faker for creating fake data\n",
    "fake = Faker()\n",
    "\n",
    "num_records = 30\n",
    "\n",
    "# List of food items to use for item_name\n",
    "food_items = [\n",
    "    \"Apple\", \"Banana\", \"Carrot\", \"Tomato\", \"Cucumber\", \"Chicken Breast\", \"Lettuce\", \"Onion\", \"Rice\", \"Pasta\",\n",
    "    \"Steak\", \"Salmon\", \"Spinach\", \"Potato\", \"Garlic\", \"Broccoli\", \"Mango\", \"Grapes\", \"Avocado\", \"Cheese\",\n",
    "    \"Yogurt\", \"Milk\", \"Butter\", \"Eggs\", \"Peanut Butter\", \"Honey\", \"Olive Oil\", \"Bread\", \"Cereal\", \"Tuna\",\n",
    "    \"Turkey\", \"Lamb\", \"Zucchini\", \"Pumpkin\", \"Beef\", \"Sausage\", \"Pineapple\", \"Kiwi\", \"Peach\", \"Pomegranate\"\n",
    "]\n",
    "\n",
    "#Create lists to store valid customer_ids, order_ids, and item_ids\n",
    "customer_ids = []\n",
    "order_ids = []\n",
    "item_ids = []\n",
    "\n",
    "# Create a set to track (order_id, item_id) combinations to prevent duplicates\n",
    "order_item_pairs = set()\n",
    "\n",
    "#Insert data into 'Customers' table\n",
    "for _ in range(num_records):\n",
    "    name = fake.name()\n",
    "    registration_date = fake.date_this_decade()  \n",
    "\n",
    "    #Random insert NULL for region\n",
    "    region = fake.city() if random.choice([True, False]) else None\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO Customers (name, registration_date, region)\n",
    "    VALUES (%s, %s, %s) RETURNING customer_id;\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_query, (name, registration_date, region))\n",
    "    customer_id = cursor.fetchone()[0]  \n",
    "    customer_ids.append(customer_id)  \n",
    "\n",
    "#Insert data into 'Items' table\n",
    "for _ in range(num_records):\n",
    "    item_name = random.choice(food_items)  \n",
    "    \n",
    "    item_price = round(fake.random_number(digits=2), 2)  \n",
    "\n",
    "    #Random insert NULL for inventory_stock\n",
    "    inventory_stock = fake.random_int(min=0, max=100) if random.choice([True, False]) else None\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO Items (item_name, item_price, inventory_stock)\n",
    "    VALUES (%s, %s, %s) RETURNING item_id;\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_query, (item_name, item_price, inventory_stock))\n",
    "    item_id = cursor.fetchone()[0]  \n",
    "    item_ids.append(item_id)  \n",
    "\n",
    "#Insert data into 'Orders' table\n",
    "for _ in range(num_records):\n",
    "    customer_id = random.choice(customer_ids)  \n",
    "    order_date = fake.date_this_year() \n",
    "\n",
    "    #Random insert NULL for total_amount\n",
    "    total_amount = round(fake.random_number(digits=2), 2) if random.choice([True, False]) else None\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO Orders (customer_id, order_date, total_amount)\n",
    "    VALUES (%s, %s, %s) RETURNING order_id;\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_query, (customer_id, order_date, total_amount))\n",
    "    order_id = cursor.fetchone()[0]  \n",
    "    order_ids.append(order_id)  \n",
    "\n",
    "#Insert data into 'Order_Items' table \n",
    "for _ in range(num_records):\n",
    "    order_id = random.choice(order_ids)  \n",
    "    item_id = random.choice(item_ids)  \n",
    "\n",
    "    while (order_id, item_id) in order_item_pairs:\n",
    "        order_id = random.choice(order_ids)  \n",
    "        item_id = random.choice(item_ids)    \n",
    "\n",
    "    order_item_pairs.add((order_id, item_id))  \n",
    "    \n",
    "    quantity = fake.random_int(min=1, max=5)\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO Order_Items (order_id, item_id, quantity)\n",
    "    VALUES (%s, %s, %s);\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_query, (order_id, item_id, quantity))\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "print(f\"{num_records} records inserted into all tables successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Process\n",
    "\n",
    "Assume you’re receiving daily files with new orders and customer updates. Write a Python\n",
    "script that would:\n",
    "- Load new data from a CSV file into a staging table.\n",
    "- Update the main tables (`Customers`, `Orders`, `Order_Items`) based on this new data.\n",
    "\n",
    "Deliverable\n",
    "- Python code demonstrating the ETL process.\n",
    "- Brief explanation of your approach to handling updates, deletions, and new entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "The script below loads a CSV file into a staging area using a single line of code with the pandas library:\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "This converts the CSV data into a DataFrame, which is more suitable for efficient data processing and manipulation. The subsequent data processing steps, including extracting relevant columns and inserting data into the database, are fully automated using Python.\n",
    "\n",
    "Given that the CSV file size is expected to be manageable (less than a few GB), this approach leverages pandas DataFrames to efficiently process the data without the overhead of executing multiple SQL queries. By processing the data entirely in-memory, the script avoids the latency introduced by querying the database multiple times, making it a more efficient solution for moderately sized datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#Function to load CSV into a Pandas DataFrame\n",
    "def load_csv_to_dataframe(csv_file_path):\n",
    "    return pd.read_csv(csv_file_path)\n",
    "\n",
    "#Function to insert data into PostgreSQL using pandas DataFrame\n",
    "def insert_data_from_df(df, table_name):\n",
    "    \n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer, index=False, header=False)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    #Insert data into the PostgreSQL table\n",
    "    cursor.copy_from(buffer, table_name, sep=',')\n",
    "    connection.commit()\n",
    "\n",
    "#Function to process and insert data into the respective tables\n",
    "def process_and_insert_data(csv_file_path):\n",
    "   \n",
    "    df = load_csv_to_dataframe(csv_file_path)\n",
    "\n",
    "    if {'customer_id', 'name', 'registration_date', 'region'}.issubset(df.columns):\n",
    "        customers_df = df[['customer_id', 'name', 'registration_date', 'region']]\n",
    "        insert_data_from_df(customers_df, 'customers')\n",
    "\n",
    "    if {'order_id', 'customer_id', 'order_date', 'total_amount'}.issubset(df.columns):\n",
    "        orders_df = df[['order_id', 'customer_id', 'order_date', 'total_amount']]\n",
    "        insert_data_from_df(orders_df, 'orders')\n",
    "    \n",
    "    if {'item_id', 'item_name', 'item_price', 'inventory_stock'}.issubset(df.columns):\n",
    "        items_df = df[['item_id', 'item_name', 'item_price', 'inventory_stock']]\n",
    "        insert_data_from_df(items_df, 'items')\n",
    "\n",
    "    if {'order_id', 'item_id', 'quantity'}.issubset(df.columns):\n",
    "        order_items_df = df[['order_id', 'item_id', 'quantity']]\n",
    "        insert_data_from_df(order_items_df, 'order_items')\n",
    "\n",
    "    print(\"ETL process completed successfully.\")\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = 'path_to_combined_csv.csv'\n",
    "\n",
    "# Execute the ETL process\n",
    "process_and_insert_data(csv_file_path)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Data Analysis and Dashboarding\n",
    "\n",
    "1. KPIs and Metrics\n",
    "Based on the schema from Task 1, calculate the following metrics:\n",
    "- Daily Total Revenue: Total amount of orders per day.\n",
    "- Average Order Value (AOV): Average order amount over the last 30 days.\n",
    "- Monthly Active Customers: Count of unique customers who placed an order within the last\n",
    "30 days.\n",
    "- Top Selling Items: The 5 most ordered items in the last 30 days.\n",
    "\n",
    "Deliverable\n",
    "- SQL queries for each metric.\n",
    "- Short explanation of each metric’s relevance to the business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "For this task, I chose to use SQL queries instead of fetching all the data and using Pandas for the EDA process. If the database were larger, storing the entire dataset in a Pandas DataFrame wouldn't be feasible. While Pandas might be more suitable for a small database like this example, I opted for a more practical approach, using SQL queries for each exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily Total Revenue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    order_date total_revenue\n",
      "0   2024-01-13         96.00\n",
      "1   2024-01-19         84.00\n",
      "2   2024-01-28          None\n",
      "3   2024-02-01         39.00\n",
      "4   2024-02-03         72.00\n",
      "5   2024-02-29         91.00\n",
      "6   2024-03-16         66.00\n",
      "7   2024-04-26         84.00\n",
      "8   2024-05-12          None\n",
      "9   2024-05-14         68.00\n",
      "10  2024-05-16         85.00\n",
      "11  2024-05-18         63.00\n",
      "12  2024-05-27         37.00\n",
      "13  2024-06-14         84.00\n",
      "14  2024-06-18          None\n",
      "15  2024-06-28         63.00\n",
      "16  2024-08-03         76.00\n",
      "17  2024-08-13          None\n",
      "18  2024-08-25         81.00\n",
      "19  2024-09-09          7.00\n",
      "20  2024-09-12          None\n",
      "21  2024-09-18          None\n",
      "22  2024-10-06          None\n",
      "23  2024-10-12          None\n",
      "24  2024-10-13         71.00\n",
      "25  2024-10-16          None\n",
      "26  2024-10-28          None\n",
      "27  2024-11-12         40.00\n",
      "\n",
      "The cumulative revenue across all days is: 1207.00\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "        order_date,\n",
    "        SUM(total_amount) AS total_revenue\n",
    "    FROM\n",
    "        Orders\n",
    "    GROUP BY\n",
    "        order_date\n",
    "    ORDER BY\n",
    "        order_date;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "#Fetch the result and convert to DataFrame\n",
    "daily_revenue = pd.DataFrame(cursor.fetchall(), columns=['order_date', 'total_revenue'])\n",
    "\n",
    "print(daily_revenue)\n",
    "\n",
    "print(f\"\\nThe cumulative revenue across all days is: {daily_revenue['total_revenue'].sum()}\")\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Order Value (AOV):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "This took me a few extra queries to visualize the data. Once I realized that the total_amount column couldn't be used due to many NaN values, I decided to apply some feature engineering and use a combination of columns (quantity + item_price) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2024, 10, 16),), (datetime.date(2024, 6, 14),), (datetime.date(2024, 10, 13),), (datetime.date(2024, 5, 16),), (datetime.date(2024, 2, 29),), (datetime.date(2024, 9, 18),), (datetime.date(2024, 4, 26),), (datetime.date(2024, 10, 12),), (datetime.date(2024, 8, 13),), (datetime.date(2024, 9, 12),)]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#SQL query for viewing order_date\n",
    "query = \"\"\"\n",
    "SELECT order_date FROM Orders LIMIT 10;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Execute the query\n",
    "cursor.execute(query)\n",
    "\n",
    "#Fetch the results\n",
    "result = cursor.fetchall()\n",
    "\n",
    "print(result)\n",
    "\n",
    "#Close the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None,), (Decimal('84.00'),), (Decimal('71.00'),), (Decimal('85.00'),), (Decimal('91.00'),), (None,), (Decimal('84.00'),), (None,), (None,), (None,)]\n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#SQL query for viewing total amount\n",
    "query = \"\"\"\n",
    "SELECT total_amount FROM Orders LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "print(result)\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Order Value (AOV) over the last 30 days: 351.5, and the average order count over the last 30 days is: 0.067\n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#SQL query for calculating Average Order Value over the last 30 days and also Average Order Count\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    AVG(order_totals.order_total) AS average_order_value,\n",
    "    COUNT(o.order_id) / 30.0 AS average_daily_order_count\n",
    "FROM (\n",
    "    -- Subquery to calculate total value of each order\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        SUM(oi.quantity * i.item_price) AS order_total\n",
    "    FROM Orders o\n",
    "    INNER JOIN Order_Items oi ON o.order_id = oi.order_id\n",
    "    INNER JOIN Items i ON oi.item_id = i.item_id\n",
    "    WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days'\n",
    "    GROUP BY o.order_id\n",
    ") AS order_totals\n",
    "INNER JOIN Orders o ON o.order_id = order_totals.order_id\n",
    "WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days';\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "#Assign each value to respective variable for printing.\n",
    "average_order_value, average_daily_order_count = result[0]\n",
    "\n",
    "print(f\"Average Order Value (AOV) over the last 30 days: {float(average_order_value)}, and the average order count over the last 30 days is: {average_daily_order_count:.3f}\")\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Active Customers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "This could be achieved by using count distinct on the customer_id and a where clause for the last 30 days interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montly Active Customers over the last 30 days: 3\n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#SQL query for calculating Montly Active Customers\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT o.customer_id) AS monthly_active_customers\n",
    "FROM Orders o\n",
    "WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days';\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "print(f\"Montly Active Customers over the last 30 days: {result[0][0]}\")\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Selling Items: The 5 most ordered items in the last 30 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "It seems that even by having several item names in my database, only 3 popped up for being ordered in the last 30 days. The sql query for this uses 2 inner joins to access the date from the orders table, quantity from the order_items table and item_name from the items table. The results are then converted into a pandas dataframe for better visualization of the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  item_name  total_quantity_sold\n",
      "0     Peach                    5\n",
      "1      Eggs                    3\n",
      "2     Mango                    2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#SQL query for calculating Top Selling Items\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    i.item_name,\n",
    "    SUM(oi.quantity) AS total_quantity_sold\n",
    "FROM Order_Items oi\n",
    "INNER JOIN Items i ON oi.item_id = i.item_id\n",
    "INNER JOIN Orders o ON oi.order_id = o.order_id\n",
    "WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days'\n",
    "GROUP BY i.item_name\n",
    "ORDER BY total_quantity_sold DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "#Fetching column names from the cursor description\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# Converting to a DataFrame\n",
    "df = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "print(df)\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Data Quality and Troubleshooting\n",
    "\n",
    "1. Data Quality Checks\n",
    "Implement SQL queries to perform the following data quality checks:\n",
    "- Check for any `NULL` values in critical columns (e.g., `order_id`, `customer_id`,\n",
    "`total_amount`).\n",
    "- Identify orders with unusually high or low `total_amount` values.\n",
    "- Ensure all `order_id` in `Order_Items` exist in the `Orders` table (foreign key integrity).\n",
    "\n",
    "Deliverable\n",
    "- SQL queries for each data quality check.\n",
    "- Brief explanation of how you would automate these checks for daily monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "First I will check all NULL values from critical columns in the orders table. Then I will display all in a single pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    order_id  customer_id total_amount\n",
      "0        182          225         None\n",
      "1        187          240         None\n",
      "2        189          227         None\n",
      "3        190          222         None\n",
      "4        191          236         None\n",
      "5        192          217         None\n",
      "6        194          229         None\n",
      "7        198          217         None\n",
      "8        199          237         None\n",
      "9        204          239         None\n",
      "10       208          237         None\n",
      "11       211          213         None\n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#SQL query for checking for NULL values in critical columns in Orders table\n",
    "query = \"\"\" \n",
    "SELECT order_id, customer_id, total_amount\n",
    "FROM Orders\n",
    "WHERE order_id IS NULL\n",
    "   OR customer_id IS NULL\n",
    "   OR total_amount IS NULL;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "#Fetching column names from the cursor description\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# Converting to a DataFrame\n",
    "df = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "print(df)\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For identifying unusual values, I first computed the total_amount by summing the quantity and item_price columns, as the total_amount column contains many NULL values. Then, I calculated the average (avg_total) and standard deviation (stddev_total) of the computed total_amount. Any orders with a total_amount more than 2 standard deviations away from the average were flagged as unusually high or low. These flagged results were then displayed in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  customer_id expected_total_amount             avg_total  \\\n",
      "0       198          217                712.00  259.3684210526315789   \n",
      "\n",
      "       stddev_total  \n",
      "0  210.305389196630  \n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#SQL query for checking for NULL values in critical columns in Orders table\n",
    "query = \"\"\" \n",
    "WITH OrderTotal AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        SUM(oi.quantity * i.item_price) AS expected_total_amount\n",
    "    FROM Orders o\n",
    "    JOIN Order_Items oi ON o.order_id = oi.order_id\n",
    "    JOIN Items i ON oi.item_id = i.item_id\n",
    "    GROUP BY o.order_id, o.customer_id\n",
    "),\n",
    "Stats AS (\n",
    "    SELECT \n",
    "        AVG(expected_total_amount) AS avg_total,\n",
    "        STDDEV(expected_total_amount) AS stddev_total\n",
    "    FROM OrderTotal\n",
    ")\n",
    "SELECT \n",
    "    ot.order_id,\n",
    "    ot.customer_id,\n",
    "    ot.expected_total_amount,\n",
    "    s.avg_total,\n",
    "    s.stddev_total\n",
    "FROM OrderTotal ot, Stats s\n",
    "WHERE ot.expected_total_amount > s.avg_total + 2 * s.stddev_total\n",
    "   OR ot.expected_total_amount < s.avg_total - 2 * s.stddev_total;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "#Fetching column names from the cursor description\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# Converting to a DataFrame\n",
    "df = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "print(df)\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we ensure all `order_id` in `Order_Items` exist in the `Orders` table (foreign key integrity) with a simple query which will return any order_id that doesnt have a correcponing entity in the orders table, ensuring integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#SQL query for checking for NULL values in critical columns in Orders table\n",
    "query = \"\"\" \n",
    "SELECT oi.order_id\n",
    "FROM Order_Items oi\n",
    "LEFT JOIN Orders o ON oi.order_id = o.order_id\n",
    "WHERE o.order_id IS NULL;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "print(result)\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Troubleshooting Data Load Issues\n",
    "Imagine a scenario where daily data loads are failing, and the new orders are not appearing\n",
    "on the dashboard. Outline a step-by-step approach for diagnosing and resolving this issue.\n",
    "Consider checkpoints in the ETL pipeline, database issues, and dashboard updates.\n",
    "\n",
    "Deliverable\n",
    "- Write-up detailing your troubleshooting process and key checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "To troubleshoot the missing daily data loads, I’d start by getting a clear picture of what’s going wrong. First, I’d check the ETL (Extract, Transform, Load) process and look through the logs for any recent changes, errors, or disruptions. I’d look for issues like changes to the ETL jobs, faulty filters, missing transformations, or failed job executions that could have caused the problem.\n",
    "\n",
    "Next, I’d make sure the data being loaded is correct and complete. This means checking for any issues like NULL values, wrong data types, or strange special characters. If those checks aren’t already set up, I’d add filters to catch these types of errors early on in the process.\n",
    "\n",
    "I’d also double-check that the database connection is set up properly. I want to make sure the pipeline is pointing to the right database and that there are no connection issues. Duplicate data or slow loading times are also important to look at, as they can point to problems with the ETL process or data transformations.\n",
    "\n",
    "The database schema is another important area to check. If there have been any changes to the schema, like renamed columns or mismatched data types, this could break the data load. I’d confirm that the schema still matches what the ETL pipeline expects, and ensure everything is properly normalized.\n",
    "\n",
    "Finally, I’d review the dashboard itself to make sure data is being displayed correctly. I’d check if there have been any changes to the data source connection, visualizations, or filters that might be causing new data to be left out.\n",
    "\n",
    "To make future troubleshooting easier, I’d create a checklist of potential issues. This would help me quickly diagnose similar problems down the road and ensure that troubleshooting steps are well-documented for consistency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
